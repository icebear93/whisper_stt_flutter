1
00:00:00,000 --> 00:00:10,000
 이 영상은 유료광고를 포함하고 있습니다.

2
00:00:30,000 --> 00:00:40,000
 안녕하세요.

3
00:00:40,000 --> 00:00:46,000
 안녕하세요.

4
00:00:46,000 --> 00:00:51,000
 안녕하세요.

5
00:00:51,000 --> 00:01:00,000
 안녕하세요.

6
00:01:00,000 --> 00:01:02,000
 진세쌤.

7
00:01:02,000 --> 00:01:04,000
 네.

8
00:01:04,000 --> 00:01:07,000
 어제 늦게 갔어요?

9
00:01:07,000 --> 00:01:10,000
 아니요. 아주 살짝만 늦게 갔어요. 왜요?

10
00:01:10,000 --> 00:01:14,000
 아 그냥요. 많이 늦게 갔나 해서.

11
00:01:14,000 --> 00:01:21,000
 아니요. 살짝만 늦게 갔어요.

12
00:01:21,000 --> 00:01:31,000
 오늘 서포터 패턴 확신인데

13
00:01:31,000 --> 00:01:33,000
 제가 먼저 스타트를 끊고

14
00:01:33,000 --> 00:02:02,000
 다른 분들 계시면 이어서 하겠습니다.

15
00:02:02,000 --> 00:02:10,000
 PPT가 보이시나요? 아니면 지구 배경 화면이 보이시나요?

16
00:02:10,000 --> 00:02:12,000
 PPT가 잘 보이네요.

17
00:02:12,000 --> 00:02:14,000
 PPT가 잘 보이나요?

18
00:02:14,000 --> 00:02:15,000
 네.

19
00:02:15,000 --> 00:02:22,000
 감사합니다.

20
00:02:22,000 --> 00:02:29,000
 이건 저번에 공지고

21
00:02:29,000 --> 00:02:34,000
 시즌 2는 여기까지 왔죠.

22
00:02:34,000 --> 00:02:51,000
 시즌 3는 거의 다 다음 달 2주차 정도부터 딥러닝이 시작해요.

23
00:02:51,000 --> 00:02:55,000
 그래서 다음 주는 정말 중요한 발리데이션.

24
00:02:56,000 --> 00:02:59,000
 얘는 알고리즘이라고 하기 보다는

25
00:02:59,000 --> 00:03:03,000
 우리가 그 알고리즘이 제대로 됐나

26
00:03:03,000 --> 00:03:05,000
 우리가 학습시킨게 제대로 됐나 그런거고요.

27
00:03:05,000 --> 00:03:11,000
 이거 보셨는지 모르겠는데

28
00:03:11,000 --> 00:03:19,000
 제가 주말에 토요일날 또 아침에 오전 10시부터 학원을 가가지고

29
00:03:19,000 --> 00:03:23,000
 조금 힘든데 혹시 다른 시간 안될지 여쭤보고 싶어요.

30
00:03:23,000 --> 00:03:30,000
 다른 시간 안될지 가능한 시간을 말씀해주셨으면 좋겠어요.

31
00:03:30,000 --> 00:03:33,000
 제 발표 끝나고.

32
00:03:33,000 --> 00:03:34,000
 죄송합니다.

33
00:03:34,000 --> 00:03:41,000
 지금 당장 바꾸는건 아니고 3월 9일 전으로 바꿨으면 좋겠습니다.

34
00:03:41,000 --> 00:03:47,000
 아니면 저는 그냥 올릴테니까 저 없이 만나셔도 되고

35
00:03:47,000 --> 00:03:51,000
 아무튼 서프트펙터 워신

36
00:03:52,000 --> 00:03:58,000
 전통적으로 서프트펙터 워신이 성능이 좋았어요.

37
00:03:58,000 --> 00:04:03,000
 딥러닝이 나타나기 전까지는 서프트펙터 워신이랑

38
00:04:03,000 --> 00:04:10,000
 그 다음에 시즌 3 그쪽 알고리즘 계열이 성능이 높고 각각 맞고

39
00:04:10,000 --> 00:04:12,000
 논문도 많이 쓰고 있었는데

40
00:04:12,000 --> 00:04:17,000
 딥러닝이 나와가지고 시들해진게 없지 않아 있지만

41
00:04:17,000 --> 00:04:24,000
 아직까지도 강력한 알고리즘입니다.

42
00:04:24,000 --> 00:04:28,000
 기본 아이디어는 데이터를 고차한 공간에 맵핑하고

43
00:04:28,000 --> 00:04:32,000
 이 공간에서 최대 마진을 갖는 결정 경계를 찾는 것입니다.

44
00:04:32,000 --> 00:04:35,000
 이 부분 조금 헷갈릴 수 있는데

45
00:04:37,000 --> 00:04:40,000
 그런 부족한 부분에 대해서는

46
00:04:40,000 --> 00:04:46,000
 제가 하이퍼링크도 걸어놨거든요.

47
00:04:46,000 --> 00:04:50,000
 나중에 제가 질화에 올리시면 참고하시면 좋을 것 같아요.

48
00:04:50,000 --> 00:04:56,000
 그래서 이건 CykinLan에서 제공하는 서프트펙터 워신에 대한 학습 정리인데

49
00:04:56,000 --> 00:04:59,000
 여기 다 나와 있어요. 서프트펙터 워신이 뭐고

50
00:04:59,000 --> 00:05:06,000
 장점이 뭐고 단점이 뭐고 보니까 괜찮더라고요.

51
00:05:07,000 --> 00:05:15,000
 지금 여기서 이렇게 1,2,4 다를 수 있는데 그렇게 하면 길어질 것 같아서 저는 그냥 생략했습니다.

52
00:05:15,000 --> 00:05:21,000
 복잡성, 팁선, practical use 이런거 얘기했는데

53
00:05:21,000 --> 00:05:27,000
 대부분 파이버파라미터를 잘 세팅하라고 하는 거죠.

54
00:05:29,000 --> 00:05:32,000
 커널 펑션도 잘 세팅을 하고

55
00:05:32,000 --> 00:05:35,000
 그리고 여기에 이게 어떤 수학적 근거가 있는지

56
00:05:35,000 --> 00:05:39,000
 여기 보면 결정경계가 딱 이 부분이에요.

57
00:05:40,000 --> 00:05:46,000
 결정경계를 수학적으로 잘 분류를 해서 얘랑 얘랑 분류가 잘 될 수 있도록

58
00:05:46,000 --> 00:05:53,000
 이 선을 그리는 게 서프트펙터 워신의 가장 큰 핵심입니다.

59
00:05:53,000 --> 00:05:56,000
 그래서 여기에 수학적인 부분도 이렇게 들어가고

60
00:05:56,000 --> 00:06:02,000
 이 부분 관심 있으시면 참고를 하시면 좋을 것 같습니다.

61
00:06:05,000 --> 00:06:10,000
 그리고 제가 그냥 대충 이렇게 뭐

62
00:06:10,000 --> 00:06:15,000
 좀 유명한 친구다 얘가 잘 쓰인다 라고 하면은 조금 싱밍성이 없잖아요.

63
00:06:15,000 --> 00:06:17,000
 그래서 다른 링크도 찾아왔는데

64
00:06:17,000 --> 00:06:22,000
 이게 바로 머신러닝을 할 때

65
00:06:22,000 --> 00:06:29,000
 약간 뭐라고 해야 되지 이런 식으로 분석을 하면은 좋다

66
00:06:29,000 --> 00:06:35,000
 이런 알고리즘을 쓰면은 데이터가 50개 이상이면은 뭐 더 가지고 오거나

67
00:06:35,000 --> 00:06:40,000
 아니거나 뭐 이런 식으로 해가지고 이렇게 쭉쭉쭉쭉

68
00:06:40,000 --> 00:06:44,000
 뭐라고 해야 되지 로드맵을 그려준 건데

69
00:06:44,000 --> 00:06:48,000
 보이시면은 SVC 보이시죠

70
00:06:48,000 --> 00:06:54,000
 그러니까 데이터가 좀 적을 때 아까 말한 것처럼 고차원 데이터로

71
00:06:54,000 --> 00:06:57,000
 특성을 분석을 하는데

72
00:06:57,000 --> 00:07:01,000
 데이터가 적을 경우에는 되게 유리하다라고

73
00:07:01,000 --> 00:07:07,000
 적혀 있어요. 서프트펙터 워신이. 그래서 여기 샘플 수가 100k 미만인 경우에는

74
00:07:07,000 --> 00:07:13,000
 서프트펙터 워신을 써라 이런 식으로도 추천이 로드맵에 나오죠

75
00:07:17,000 --> 00:07:21,000
 그래서 여기 링크를 통해서 정리하고 코드를 통해서 저는

76
00:07:21,000 --> 00:07:26,000
 뭐 어려운 거 없어요. 늘 그랬듯이 iris 데이터를 통해서

77
00:07:26,000 --> 00:07:34,000
 랜턴 스테이트 주고 테스트 사이즈를 25%를 테스트를 하기 위해서 25%를 설정을 하고

78
00:07:34,000 --> 00:07:39,000
 SVC 그리고 저는 라이뉴어로 했어요. 라이뉴어

79
00:07:48,000 --> 00:07:52,000
 이렇게 이렇게 분류를 했어요. 라이뉴어

80
00:07:52,000 --> 00:07:55,000
 그 뭐지

81
00:07:56,000 --> 00:08:04,000
 아, with 라이뉴어이니까 요번에. 그 북것을 이런 식으로 분류를 했어요

82
00:08:04,000 --> 00:08:12,000
 그래서 테스트 해보니까. 아 그 모델을 돌려보니까 0.97%로 되게 높은 확률이 나왔구요

83
00:08:12,000 --> 00:08:17,000
 제가 이게 이 정밀도에 대해 정밀도, 리퀄, f1치고 서포트 이런 거에 대해서

84
00:08:17,000 --> 00:08:22,000
 맥크로 에버지 웨이딧 에버지 이런 거 다 여기 다 정리를 해놨거든요 저도

85
00:08:22,000 --> 00:08:28,000
 보니까 되게 중요한 개념이더라구요. 이런 게 통계학적으로나 뭐 여러 가지로

86
00:08:28,000 --> 00:08:34,000
 그래서 만약에 관심 있으신 분들 정리한 것도 참고를 하셔서 보시면 좋을 것 같구요

87
00:08:34,000 --> 00:08:39,000
 아무튼 되게 높게 나왔다. 정밀도가 굉장히 높다.

88
00:08:39,000 --> 00:08:43,000
 그래서 실제로 이렇게 대충 보면 이해가 안 될 거니까

89
00:08:44,000 --> 00:08:51,000
 저희가 늘 쓴 iris 데이터는 0.1.2로 분류가 되죠. 0.1.2 숫자로

90
00:08:51,000 --> 00:08:59,000
 그래서 얘는 서포트 벡터 머신이 0.1.2를, 꽃의 종류가 0.1.2로 분류가 되는데

91
00:08:59,000 --> 00:09:07,000
 얘는 그 서포트 벡터 머신이 학습을 하고 이제 0.1.2 중에서 예측을 한 경우를

92
00:09:07,000 --> 00:09:12,000
 지금 이렇게 보시는, 지금 제가 시각화를 했어요. 그래서 보시면 거의 대부분 맞죠

93
00:09:12,000 --> 00:09:20,000
 아 이거 딱 틀린 거 제외하고는 거의 다 맞죠. 그래서 97%가 나온 것 같아요

94
00:09:20,000 --> 00:09:25,000
 저는 여기까지 이렇게 서포트 벡터 머신 공부를 해봤습니다

95
00:09:27,000 --> 00:09:38,000
 질문 있으시면 주시고 없으시면 바로 다음 분들에게

96
00:09:38,000 --> 00:09:40,000
 네

97
00:09:44,000 --> 00:09:48,000
 다음 타자 있으신가요?

98
00:10:02,000 --> 00:10:04,000
 오늘은 안 계신가요?

99
00:10:04,000 --> 00:10:06,000
 저한테요? 끝이에요?

100
00:10:06,000 --> 00:10:08,000
 아 그래요?

101
00:10:08,000 --> 00:10:10,000
 오늘은 굉장히 빠르고

102
00:10:10,000 --> 00:10:12,000
 중간에 끊겼는데

103
00:10:14,000 --> 00:10:22,000
 시간이 부족해서 좀 하다가 발표하기에는 좀 부족한 것 같아요

104
00:10:22,000 --> 00:10:28,000
 괜찮습니다. 그런 거는 원래 여기는 다른 사람들이 뭐 공부했나

105
00:10:28,000 --> 00:10:32,000
 본인이 뭐를 공부했나라고 발표하는 시간이기 때문에

106
00:10:32,000 --> 00:10:34,000
 뭐 저는 늘

107
00:10:34,000 --> 00:10:36,000
 네?

108
00:10:36,000 --> 00:10:38,000
 코드도 못 찾는데 해요?

109
00:10:38,000 --> 00:10:40,000
 아무도 준비 안 했으니까 내가 해야 될 것 같은데

110
00:10:40,000 --> 00:10:42,000
 역시 과장님

111
00:10:44,000 --> 00:10:50,000
 아 그냥 뭐 진짜 좀 이번 거는 허접해서 그냥 제가 본대까지만 그냥

112
00:10:50,000 --> 00:10:52,000
 그럼 그 정도만 말씀드릴게요

113
00:10:52,000 --> 00:10:58,000
 저도 ppt 만들다가 이거 도저히 안 될 것 같아가지고

114
00:10:58,000 --> 00:11:00,000
 이번주에 술략도 나오실 것 같고

115
00:11:00,000 --> 00:11:08,000
 그럼 일단 제가 뭐 그냥 본거 그 정도만 일단 할게요

116
00:11:08,000 --> 00:11:10,000
 네 아유 그것도 좋죠

117
00:11:18,000 --> 00:11:20,000
 저는 뭐 코드도 못 찾고 아직

118
00:11:20,000 --> 00:11:22,000
 일단 제가 좀 알아봤는데

119
00:11:22,000 --> 00:11:26,000
 그 뭐 서포터 팩터 머신 관련해서 알아보다 보니까

120
00:11:26,000 --> 00:11:34,000
 뭐 이게 그 전에 약간 원조적인 개념이 퍼셉트론이라는 개념이 있더라구요

121
00:11:34,000 --> 00:11:40,000
 그래서 뭐 이제 이게 발전한 게 대표적으로 서포터 팩터 머신이 있다 뭐 이런 설명이 있어서

122
00:11:40,000 --> 00:11:44,000
 퍼셉트론을 그냥 간략하게 좀 알아봤었는데

123
00:11:44,000 --> 00:11:52,000
 인간의 그 뉴런 뇌신경계를 이제 이런 식으로 모방을 해가지고

124
00:11:52,000 --> 00:11:54,000
 이제 뭐 한다

125
00:11:56,000 --> 00:11:58,000
 라고 했는데

126
00:11:58,000 --> 00:12:02,000
 뭐 여기서 이렇게 지금 표에 나왔듯이

127
00:12:04,000 --> 00:12:06,000
 여기 섭트로는

128
00:12:06,000 --> 00:12:10,000
 여기 셀 바디가 이제 노드로 교체되고

129
00:12:10,000 --> 00:12:16,000
 그 다음에 가지 돌기가 이제 입력갑

130
00:12:16,000 --> 00:12:22,000
 그 다음에 여기 요렇게 이 연결되는 부분

131
00:12:22,000 --> 00:12:26,000
 이 스냅스 셀들의 이 집합

132
00:12:26,000 --> 00:12:30,000
 그 다음에 여기 이 노드가 이제 입력갑

133
00:12:30,000 --> 00:12:32,000
 연결되는 부분

134
00:12:32,000 --> 00:12:36,000
 이 스냅스 셀들의 이 집합

135
00:12:36,000 --> 00:12:42,000
 이게 이제 가중치 변수에 붙는 가중치라고 보시면 될 것 같고

136
00:12:42,000 --> 00:12:48,000
 그 다음에 이제 요기 요 부분이 축작인데

137
00:12:48,000 --> 00:12:54,000
 이게 이제 풀력이다 이런 형태로 인공 뉴런을 만들어서

138
00:12:54,000 --> 00:12:58,000
 이제 아티피셜 뉴런이라고 하고

139
00:12:58,000 --> 00:13:04,000
 이 퍼셉트론 개념이 이런 것들을 담고 있다 이런 내용이었고

140
00:13:04,000 --> 00:13:09,000
 그래서 이거는 로젠블렛이라는 사람이 1957년

141
00:13:09,000 --> 00:13:14,000
 엄청 오래 사는데 그때 보완했던 알고리즘인데

142
00:13:14,000 --> 00:13:19,000
 이런 식으로 입력데이터 두 개의 분류 중 하나로 분류하는 분류기이다

143
00:13:19,000 --> 00:13:23,000
 원시적인 신경망이라고 볼 수 있다

144
00:13:23,000 --> 00:13:26,000
 뭐 이 정도로 그냥 찾아봤어도

145
00:13:27,000 --> 00:13:35,000
 음 그냥 그 나왔던 내용이 뭐 결국엔 이것도 분류를 하는 건데

146
00:13:35,000 --> 00:13:47,000
 이전에 했던 클래스들을 나누는 경계선을 찾기 위해서

147
00:13:47,000 --> 00:13:52,000
 사용하는 방분론 중에 하나가 서포트 벡터 머신이고

148
00:13:52,000 --> 00:13:58,000
 이 서포트 벡터 머신은 이런 식으로

149
00:13:58,000 --> 00:14:08,000
 학습 데이터가 이런 식으로 있으면 판별 함수 y가 플러스 1, 마이너스 1 두 개의 값을 가질 때

150
00:14:08,000 --> 00:14:17,000
 이런 식으로 w는 가중치고 x가 변수고

151
00:14:18,000 --> 00:14:23,000
 이런 식으로 함수를 만들고 x 플러스는 플러스 1일 때

152
00:14:23,000 --> 00:14:30,000
 fx 마이너스는 마이너스 1일 때

153
00:14:30,000 --> 00:14:34,000
 그래서 이런 식으로 표현을 한 다음에

154
00:14:34,000 --> 00:14:40,000
 이거를 더 확장시켜서 밑에 내려가면 이렇게 수식으로 만들고

155
00:14:41,000 --> 00:14:47,000
 이렇게 그 이거는 제가 폴 썼는데

156
00:14:47,000 --> 00:14:51,000
 함수 그냥 그으면 이렇게 나오는데 예제로 제가 함수 그었어요

157
00:14:51,000 --> 00:14:53,000
 이 두 개 원래 겹쳐지는 건데

158
00:14:53,000 --> 00:14:58,000
 그래서 이 지금 왼쪽과 오른쪽의 함수가 한 곳에 나타나서

159
00:14:58,000 --> 00:15:08,000
 이 둘의 경계선이 멀리 가면 갈수록 서포트 벡터 머신의 성능이 좋다고 판단할 수 있다

160
00:15:09,000 --> 00:15:14,000
 뭐 그런 거 없고 밑에는 그냥 이렇게 뭐 거리부하더라고요

161
00:15:14,000 --> 00:15:20,000
 마진이라고 부르던데 이런 식으로 그냥 거리부하는 일반적인 식이고

162
00:15:20,000 --> 00:15:26,000
 여기까지 보고 저는 이제 더 봐야 되는데 이상은 제가 못 보겠어요

163
00:15:26,000 --> 00:15:33,000
 그래서 발표를 안 하려고 하다가 그래도 어디까지 했는지 정도 얘기할 시간이 있는 것 같아서

164
00:15:33,000 --> 00:15:37,000
 여기까지는 말씀드렸습니다

165
00:15:38,000 --> 00:15:42,000
 어제 술 먹어서 어쨌든 준비를 좀 잘못했네요

166
00:15:42,000 --> 00:15:46,000
 다 하셨는데요?

167
00:15:46,000 --> 00:15:52,000
 네 이상입니다

168
00:15:52,000 --> 00:16:02,000
 네 감사합니다 퍼셉트론이 되게 그 되게 오래된 그 알고리즘이 딥러닝의 시초인데

169
00:16:02,000 --> 00:16:07,000
 저는 몰랐어요 서포트 벡터 머신이 퍼셉트론의 결정 경계를 추가해가지고

170
00:16:07,000 --> 00:16:12,000
 형성한 건지는 몰랐거든요 처음에는 잘 못 들었는데

171
00:16:12,000 --> 00:16:16,000
 여기서 또 이렇게 듣게 되네요 감사합니다

172
00:16:24,000 --> 00:16:29,000
 또 어떻게 013 또 발표하시겠어요? 아니면 그냥

173
00:16:30,000 --> 00:16:37,000
 일단 그러면은 하겠습니다

174
00:16:37,000 --> 00:16:40,000
 네

175
00:16:46,000 --> 00:16:52,000
 저도 끝까지 가질 못하는 느낌이었어 가지고

176
00:16:52,000 --> 00:16:56,000
 일단 ppt로 봤대

177
00:16:57,000 --> 00:17:05,000
 기본적인 용어라고 해야 될까요? 이론이나 개념? 그런 거 잡고 가자면은

178
00:17:05,000 --> 00:17:12,000
 이렇게 데이터가 별하고 네모가 있으면은 이거를 이제

179
00:17:12,000 --> 00:17:18,000
 하나의 그룹하고 또 하나의 그룹 있잖아요 그게 이제

180
00:17:18,000 --> 00:17:28,000
 공간을 이제 둘로 나누는 평면 상황에서 이게 3차원에서는 이제 평면이 되고 2차원에서는 직선이 되는데

181
00:17:28,000 --> 00:17:36,000
 그걸 쪽평면이라고 하는데 이게 이제 두 그룹을 분류해주는 그런

182
00:17:36,000 --> 00:17:42,000
 용어 이게 되게 중요한 용어인 것 같더라고요 svm에서

183
00:17:43,000 --> 00:17:51,000
 그래서 지금 쪽평면이 3개가 있는데 이 평면에서 직선이 되는

184
00:17:51,000 --> 00:18:00,000
 이 3개의 쪽평면 중에서 여기서 제일 좋은 분류 평면은 이 초록색에 해당되는 쪽평면의

185
00:18:00,000 --> 00:18:04,000
 라고 생각을 하면 되고

186
00:18:05,500 --> 00:18:11,000
 네 이렇게 딱 그룹을 나눴을 때

187
00:18:13,000 --> 00:18:25,000
 그 별이 서로의 그 경계 그룹에 속하지 않고 100% 아예 딱 모아서 떨어졌을 때 이제 이거는 하드마진이라고

188
00:18:25,000 --> 00:18:31,000
 이런 식으로 여기 사각형 있고 여기는 별표가 있는데 서로

189
00:18:32,000 --> 00:18:45,000
 이거는 이제 잡음이라고 해가지고 조금 유연하게 가겠다 그럼 이거는 하드마진이 아니라 소프트 마진이라고

190
00:18:45,000 --> 00:18:56,000
 합니다 뭐 일단 용어는 이렇게 정리를 했고 저는 코드를 짰었습니다 잠시만요

191
00:19:01,000 --> 00:19:22,000
 보이시나요

192
00:19:23,000 --> 00:19:29,000
 네 잘 보여요 저는 단순하게 태그가 있어서

193
00:19:29,000 --> 00:19:42,000
 그 github 가 있거든요 github 주소에 누군가의 csv 파일을 올렸어요 그래가지고 이 csv 파일을 그대로 가져와서 데이터 프레임으로 변환해서

194
00:19:42,000 --> 00:19:50,000
 한쪽 그룹은 이제 색깔이 붉은색으로 한쪽 그룹은

195
00:19:52,000 --> 00:20:02,000
 그린으로 이렇게 했습니다 잠시만요

196
00:20:02,000 --> 00:20:23,000
 했을 때 이 코드를 실행해서 이제 한쪽 그룹은 레드 한쪽 그룹은 이제 그린으로 했을 때 이렇게 됐습니다

197
00:20:23,000 --> 00:20:30,000
 이 데이터는 보면 뭔가 0

198
00:20:30,000 --> 00:20:37,000
 x축이 0으로 갈라져 있죠

199
00:20:37,000 --> 00:20:44,000
 이런 데이터를 이제 학습을 시켰는데 학습시킨 코드는

200
00:20:44,000 --> 00:21:07,000
 여기 있습니다 저는 리니어 서포트 벡터 이거를 썼고

201
00:21:07,000 --> 00:21:21,000
 지금 이거는 되게 익숙할 거에요 학습시키기 전에 독립 변수와 중속 변수 그리고 이거는 이제 오류 허용 정도를 조절하고

202
00:21:21,000 --> 00:21:34,000
 이 loss의 그런 알고리즘을 힌지로 사용한다 그 깊게는 하진 않았고 이 데이터를 이제 x와 y에 대해서

203
00:21:34,000 --> 00:21:42,000
 학습을 시키고 이거는 이제 그 제가 임의적으로 데이터를 넣어서

204
00:21:42,000 --> 00:21:47,000
 결과를 예측하는 그런 상황입니다

205
00:21:54,000 --> 00:21:57,000
 나왔나?

206
00:21:57,000 --> 00:22:12,000
 아 지금 보면은 그 왼쪽 그룹은 1이고 오른쪽 그룹은 0이었어요 그래서 지금

207
00:22:12,000 --> 00:22:28,000
 마이너스 5에 마이너스 5 데이터를 넣은게 이제 왼쪽 그룹에 들어가 있다 그리고 0에 가까운 그런 세세하게 그렇게 했을 때 이렇게 결과가 나왔고

208
00:22:28,000 --> 00:22:38,000
 이거는 이제 코드를 실행했을 때 뭐 단순 경고문이기 때문에 무시해도 됩니다

209
00:22:38,000 --> 00:22:46,000
 만약 이거를 없애고 싶다면 저번에 했었는데 warning 이라는

210
00:22:46,760 --> 00:22:53,000
 그게 있었던 것 같은데 잠시만요 처리 못할 수도 있습니다

211
00:22:53,760 --> 00:23:00,000
 이 라이브러리에서 이제 필터 warning

212
00:23:09,000 --> 00:23:14,000
 이렇게 하면 이제 warning 사라지는 것도 올 수 있고

213
00:23:14,000 --> 00:23:23,000
 이건 python 그런거고 네 여기까지 했었습니다 저는 추가적으로 비단형도 해보고

214
00:23:23,000 --> 00:23:31,000
 뭐 하려 했는데 일단 잘 이해가 안 되는 것 같아 가지고 제가 발표 여기까지 하겠습니다

215
00:23:31,000 --> 00:23:41,000
 감사합니다 아니 다들 다 다 다 준비 잘 하셨는데 약간 너무 완벽하게 하시려고 하신건가

216
00:23:41,000 --> 00:23:47,000
 저는 되게 잘 봤습니다 되게 그냥 조금 그림으로도 되게 잘 이해가 됐고요

217
00:23:47,000 --> 00:23:54,000
 개념적으로라도 되게 잘 설명을 해주신 것 같아요 감사합니다

218
00:23:54,000 --> 00:23:58,000
 또 발표하실 분 없으신가요

219
00:24:04,000 --> 00:24:07,000
 저희 그

220
00:24:08,000 --> 00:24:11,000
 마무리 하기 전에

221
00:24:16,000 --> 00:24:19,000
 마무리 하기 전에

222
00:24:19,000 --> 00:24:25,000
 성재쌤이 카톡에 kegg을 올려줬거든요

223
00:24:25,000 --> 00:24:33,000
 kegg을 그거는 아직 저희가 지금 딥러닝을 하기 시작하고 나서부터 한번 저희끼리 할 수 있는 사람을 모여가지고

224
00:24:33,000 --> 00:24:41,000
 미니 프로젝트처럼 거기서부터는 진짜로 뭔가가 성과가 나오고 그런 거니까 욕심 있으신 분들은 같이 하면 좋을 것 같아요

225
00:24:41,000 --> 00:24:47,000
 그리고 제가 이제 마지막으로 가시기 전에

226
00:24:48,000 --> 00:24:55,000
 시간 변경에 대해서 한번 공지를 제가 차후에 또 띄우도록 하겠습니다

227
00:24:55,000 --> 00:25:01,000
 만약에 이 시간이 좋다 하시면은 뭐 자유롭게 이 시간에 참여하셔도 되고

228
00:25:01,000 --> 00:25:10,000
 금요일 저녁이나 아니면 일요일 오전 아니면 오후 이렇게 해서 변경이 됐으면 좋겠어요 개인적으로 저는 참여를 하고 싶어 가지고

229
00:25:10,000 --> 00:25:12,000
 어쨌든

230
00:25:12,000 --> 00:25:15,000
 이제 제가 오늘 이렇게 마무리 하겠습니다

231
00:25:15,000 --> 00:25:17,000
 감사합니다

232
00:25:17,000 --> 00:25:19,000
 감사합니다

233
00:25:19,000 --> 00:25:21,000
 감사합니다

234
00:25:21,000 --> 00:25:23,000
 감사합니다

235
00:25:23,000 --> 00:25:25,000
 감사합니다

236
00:25:25,000 --> 00:25:27,000
 감사합니다

237
00:25:27,000 --> 00:25:29,000
 감사합니다

238
00:25:30,000 --> 00:25:32,000
 아무튼

239
00:25:33,000 --> 00:25:37,000
 여기까지 하고 다음주에 또 뵙겠습니다

240
00:25:37,000 --> 00:25:39,000
 공지 또 올릴게요

241
00:25:39,000 --> 00:25:41,000
 고생하셨습니다

242
00:25:41,000 --> 00:25:43,000
 모두 고생하셨어요 진짜 다 준비 잘해주셨죠

243
00:25:43,000 --> 00:25:45,000
 네

244
00:25:50,000 --> 00:25:52,000
 네 저희 마무리 되는거 맞죠?

245
00:25:52,000 --> 00:25:54,000
 네 고생하셨습니다

246
00:25:54,000 --> 00:25:56,000
 주말 잘 보내세요

247
00:25:56,000 --> 00:25:58,000
 네 행복한 주말 되세요

248
00:25:58,000 --> 00:26:00,000
 행복한 주말 되세요

249
00:26:00,000 --> 00:26:02,000
 안녕

